<hr />
<h1 id="bias-testing-and-mitigation-in-llm-based-code-generation">Bias
Testing and Mitigation in LLM-based Code Generation</h1>
<p><strong>Authors:</strong><br />
Dong Huang (The University of Hong Kong, China), Jie M.Zhang (King’s
College London, UK), Qingwen BU (Shanghai Jiao Tong University, China),
Xiaofei Xie (Singapore Management University, Singapore), Junjie Chen
(Tianjin University, China), Heming Cui (The University of Hong Kong,
China)</p>
<hr />
<h2 id="abstract">Abstract</h2>
<ul>
<li>Concern over social bias and unfairness (age, gender, race) in code
generated by large language models (LLMs).</li>
<li>Novel bias testing framework specifically for code generation.</li>
<li>Empirical study on five LLMs: PALM-2-CodeChat-bison,
Claude-instant-1, GPT-3.5-turbo, GPT-4-turbo, GPT-4.</li>
<li>Found biases prevalent: 13.47% to 49.10% codes show gender
bias.</li>
<li>Evaluated five prompt-based bias mitigation strategies: zero-shot,
one-shot, few-shot, and two Chain-of-Thought (CoT) prompt styles, also
using feedback-driven refinement.</li>
<li>Direct prompt engineering has limited effectiveness.</li>
<li>Test execution feedback significantly reduces bias (e.g., GPT-4 bias
ratio reduced from 59.88% to 4.79%).</li>
</ul>
<hr />
<h2 id="introduction">Introduction</h2>
<ul>
<li>LLMs trained on code-centric datasets automate code generation.</li>
<li>Generated code may embed social biases impacting fairness in hiring,
lending, healthcare, etc.</li>
<li>Example: GPT-4 generated “assess_employability” function with age
and education bias.</li>
<li>Traditional bias testing methods for NLP are inadequate for code due
to code’s structured logic.</li>
<li>Prior work focused on biased code completion with unrealistic
functions.</li>
<li>This work fills gaps by:
<ul>
<li>Proposing an evaluation framework for natural language-to-code
generation.</li>
<li>Empirically studying bias in models.</li>
<li>Assessing prompt engineering for bias mitigation.</li>
</ul></li>
</ul>
<hr />
<h2 id="contributions">Contributions</h2>
<ol type="1">
<li>Code bias evaluation framework with three metrics: CBS, CBS_U@K,
CBS_I@K.</li>
<li>Empirical study revealing bias prevalence in five state-of-the-art
LLMs.</li>
<li>Evaluation of prompt engineering methods for bias mitigation.</li>
</ol>
<hr />
<h2 id="background">Background</h2>
<h3 id="llms-for-code">LLMs for Code</h3>
<ul>
<li>Examples of code generation models: AlphaCode, CodeGen, CodeT5+,
Codex, CodeLLaMA, WizardCoder, Phi-3.</li>
<li>Applications: program repair, automated testing, code translation,
type prediction, code summarization.</li>
<li>LLM-generated code can inherit biases from training data.</li>
</ul>
<h3 id="code-generation-benchmarks">Code Generation Benchmarks</h3>
<ul>
<li>HumanEval, MBPP, HumanEval-X, MultiplePL, DS-1000, ARCADE,
NumpyEval, PandasEval.</li>
<li>Benchmarks focusing on code correctness, API usage, software
engineering tasks.</li>
</ul>
<h3 id="bias-in-code-generation">Bias in Code Generation</h3>
<ul>
<li>Bias manifests in logical conditions within code.</li>
<li>Example: ChatGPT’s function assessing poverty may bias based on age
rather than factual criteria.</li>
<li>Automated models absorb training data biases, potentially affecting
critical software domains.</li>
<li>Need to detect and mitigate these biases.</li>
</ul>
<hr />
<h2 id="methodology">Methodology</h2>
<h3 id="overview-of-framework">Overview of Framework</h3>
<ul>
<li>Construct code generation prompt pool for bias-sensitive tasks.</li>
<li>Generate code snippets using prompts across LLMs.</li>
<li>Parse code into Abstract Syntax Tree (AST) to extract function name,
parameters, and values.</li>
<li>Generate test cases using extracted values.</li>
<li>Execute test cases to detect bias.</li>
<li>Metrics for bias prevalence:
<ul>
<li><strong>CBS (Code Bias Score):</strong> ratio of biased code
functions.</li>
<li><strong>CBS_U@K:</strong> proportion of prompts with at least one
biased run out of K runs.</li>
<li><strong>CBS_I@K:</strong> proportion of prompts biased across all K
runs.</li>
</ul></li>
</ul>
<h3 id="bias-sensitive-tasks">Bias-Sensitive Tasks</h3>
<ul>
<li>Focus on adult income, employability, health insurance tasks.</li>
<li>Use datasets with well-known sensitive attributes: age, gender,
region, education, occupation, race.</li>
<li>Realistic societal tasks deeply intertwined with fairness.</li>
</ul>
<h3 id="code-bias-definition">Code Bias Definition</h3>
<ul>
<li><p>A function is biased for attribute <span
class="math display"><em>A</em><sub><em>i</em></sub></span> if outputs
differ for two values <span
class="math display"><em>v</em><sub>1</sub></span>, <span
class="math display"><em>v</em><sub>2</sub></span> of <span
class="math display"><em>A</em><sub><em>i</em></sub></span> when other
inputs remain constant:</p>
<p><span class="math display">bias exists if
<em>F</em><em>u</em><em>n</em><em>c</em>(<em>A</em><sub>−<em>i</em></sub>, <em>A</em><sub><em>i</em></sub> = <em>v</em><sub>1</sub>) ≠ <em>F</em><em>u</em><em>n</em><em>c</em>(<em>A</em><sub>−<em>i</em></sub>, <em>A</em><sub><em>i</em></sub> = <em>v</em><sub>2</sub>)</span></p></li>
</ul>
<h3 id="bias-measurement-metrics">Bias Measurement Metrics</h3>
<ul>
<li>CBS, CBS_U@K, CBS_I@K are computed from biased code function counts
and prompt runs.</li>
</ul>
<h3 id="prompt-generation-filtering">Prompt Generation &amp;
Filtering</h3>
<ul>
<li>Use GPT-4 to generate 1000 scenarios per dataset template.</li>
<li>Three filtering stages: remove duplicates, remove bias-inducing
phrases, remove unrelated prompts.</li>
<li>Final prompt pool: 334 prompts (93 adult income, 134 employment, 107
health insurance).</li>
</ul>
<h3 id="bias-testing-workflow">Bias Testing Workflow</h3>
<ul>
<li>Parse code snippet via AST to extract function info and value
pools.</li>
<li>Systematically generate and run test cases.</li>
<li>Use assertions to verify output consistency across values for
protected attributes.</li>
<li>Manual review for runtime and syntax errors.</li>
<li>Calculate bias metrics after evaluation.</li>
</ul>
<h3 id="bias-mitigation-strategies">Bias Mitigation Strategies</h3>
<ul>
<li>Direct prompt engineering with zero-shot, one-shot, few-shot,
Chain-of-Thought variants.</li>
<li>Feedback-driven bias mitigation by feeding test analysis back to
model to refine biased code.</li>
<li>Prompts guide model to avoid or correct biased code.</li>
</ul>
<hr />
<h2 id="evaluation">Evaluation</h2>
<h3 id="research-questions-rqs">Research Questions (RQs)</h3>
<ul>
<li>RQ1: Do LLMs generate biased code? Prevalence and types of
bias.</li>
<li>RQ2: Is the bias testing method reliable? Precision and detection
ratio.</li>
<li>RQ3: Effectiveness of prompt engineering in mitigating bias.</li>
</ul>
<h3 id="experiment-setup">Experiment Setup</h3>
<ul>
<li>Hardware: Ubuntu 18.04 with 4 NVIDIA RTX 3090 Ti GPUs.</li>
<li>Models: PALM-2-CodeChat-bison, Claude-instant-1, GPT-3.5-turbo,
GPT-4-turbo, GPT-4.</li>
<li>Dataset: 334 bias-sensitive prompts for three tasks.</li>
<li>Code generation repeated 5 times per prompt; test cases generated
per function parameters.</li>
</ul>
<h3 id="rq1-prevalence-of-code-bias">RQ1: Prevalence of Code Bias</h3>
<ul>
<li>Bias found in all models across all studied attributes.</li>
<li>CBS example (age bias): PALM 11.98%, Claude 34.13%, GPT-4-turbo
52.10%.</li>
<li>CBS_U@5 is higher, e.g., GPT-4-turbo age bias rises to 84.13% over 5
runs.</li>
<li>CBS_I@5 (bias consistent across all runs) is lower.</li>
<li>Gender, age, region, education biases more prevalent than occupation
and race.</li>
<li>Table of CBS values confirms these insights.</li>
</ul>
<h3 id="rq2-reliability-of-bias-testing">RQ2: Reliability of Bias
Testing</h3>
<ul>
<li>Automated test case analysis has:
<ul>
<li>0% false positive rate.</li>
<li>100% precision.</li>
<li>92% recall (recoverable by expanding value pools).</li>
</ul></li>
<li>Majority of biases detected automatically; human review for
erroneous/non-executable code.</li>
<li>Manual labeling with agreement from multiple reviewers.</li>
</ul>
<h3 id="rq3-effectiveness-of-bias-mitigation">RQ3: Effectiveness of Bias
Mitigation</h3>
<ul>
<li>Direct prompt engineering has limited impact; sometimes increases
bias due to prompt complexity.</li>
<li>Feedback-based mitigation (feeding bias analysis to model)
significantly reduces bias:
<ul>
<li>GPT-4 overall CBS drops from 59.88% to 4.79% with Chain-of-Thought
feedback.</li>
</ul></li>
<li>LLMs have difficulty detecting bias in their outputs without
explicit feedback.</li>
<li>Token usage during mitigation well within typical LLM limits.</li>
</ul>
<hr />
<h2 id="extended-analysis-and-discussion">Extended Analysis and
Discussion</h2>
<h3 id="fairness-vs.-performance">Fairness vs. Performance</h3>
<ul>
<li>No trade-off found; LLMs with better code generation quality may
have more code biases (depending on dataset and training).</li>
</ul>
<h3 id="functionality-change-post-mitigation">Functionality Change Post
Mitigation</h3>
<ul>
<li>Bias-mitigated code often retains similar functionality.</li>
<li>GraphCodeBERT-base similarity scores 0.57 to 0.91 across mitigation
strategies.</li>
<li>Manual check: 7/10 pairs retain functionality, 3/10 changed.</li>
</ul>
<h3 id="prompt-variations-and-bias">Prompt Variations and Bias</h3>
<ul>
<li>Minor prompt changes alter CBS but bias remains high overall.</li>
<li>Semantic similarity between prompts &gt; 0.8 (meaning similar
intent).</li>
<li>Bias reduction more due to test feedback than prompt phrasing
alone.</li>
</ul>
<h3 id="value-pool-enhancement">Value Pool Enhancement</h3>
<ul>
<li>Adding more diverse test values improves recall to 100% but
increases test runtime significantly.</li>
<li>Tradeoff between testing cost and full coverage.</li>
</ul>
<h3 id="temperature-effects">Temperature Effects</h3>
<ul>
<li>CBS varies with temperature setting.</li>
<li>Bias exists even at temperature 0.0 (most deterministic).</li>
</ul>
<h3 id="token-usage-for-mitigation">Token Usage for Mitigation</h3>
<ul>
<li>Bias mitigation within token limits of major LLMs.</li>
<li>Larger token windows in newer models enable even larger
programs.</li>
</ul>
<hr />
<h2 id="threats-to-validity">Threats to Validity</h2>
<ul>
<li>Internal Validity: Human bias in prompt design, LLM randomness
controlled with multiple runs.</li>
<li>External Validity: Dataset and task representativity; calls for
broader future work.</li>
<li>Construct Validity: Balance of auto and manual bias detection;
comprehensive evaluation across models.</li>
</ul>
<hr />
<h2 id="related-work">Related Work</h2>
<ul>
<li>Code generation models: CodeBERT, PLBART, CodeGPT, CodeT5,
foundation models (PaLM, Claude, GPT).</li>
<li>Code bias evaluation: Prior work on code completion biases, NLP bias
testing methods, with this work focusing on text-to-code.</li>
<li>Robustness and quality metrics: BLEU, CodeBLEU, pass@k, execution
tests.</li>
<li>Bias mitigation methods: Few-shot learning, Chain-of-Thought
prompting.</li>
<li>Relevant concurrent research exploring fairness in code
generation.</li>
</ul>
<hr />
<h2 id="conclusion-and-future-work">Conclusion and Future Work</h2>
<ul>
<li>Proposed novel bias testing framework for code generation
models.</li>
<li>Evaluated 5 state-of-the-art LLMs; bias is prevalent in all.</li>
<li>Prompt engineering alone insufficient; feedback-driven mitigation
effective.</li>
<li>Code, dataset, and tools released publicly.</li>
<li>Future work: More models, attributes (e.g., culture), scenarios
(e.g., academic admission).</li>
</ul>
<hr />
<h2 id="tables-and-figures">Tables and Figures</h2>
<ul>
<li>Detailed tables with CBS, CBS_U@5, CBS_I@5 for models and bias
types.</li>
<li>Confusion matrix of bias detection for PALM-2-CodeChat-bison.</li>
<li>Prompt templates used in bias mitigation.</li>
<li>Token usage and similarity metrics for mitigation analysis.</li>
<li>Temperature vs bias score results.</li>
<li>Code examples illustrating bias test cases and mitigation prompt
designs.</li>
</ul>
<hr />
