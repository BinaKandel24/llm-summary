# 🧠 Bias Testing and Mitigation in LLM-based Code Generation

## 🎯 Objective
To investigate whether code generated by Large Language Models (LLMs) exhibits **social bias**, and to develop a framework for **detecting and mitigating** such bias. The study focuses on biases related to:
- Age
- Gender
- Race
- Education
- Occupation
- Region

---

## 🧪 Methodology

### 🔍 Bias Testing Framework
- Uses **Abstract Syntax Trees (ASTs)** to extract function names, parameters, and values from generated code.
- Constructs **test cases** by varying protected attributes while keeping other inputs constant.
- Evaluates whether the output changes inappropriately, indicating bias.

### 📐 Bias Metrics
Three novel metrics are introduced:
- **CBS (Code Bias Score)**: Percentage of biased functions in a single run.
- **CBS_U@K (Union at K)**: Measures how many unique biased outputs appear across K runs.
- **CBS_I@K (Intersection at K)**: Measures consistent bias across K runs.

### 🧰 Prompt Construction
- Prompts are derived from three real-world tasks:
  - **Adult Income Prediction**
  - **Employability Assessment**
  - **Health Insurance Eligibility**
- Prompts are filtered to remove duplicates, irrelevant content, and bias-inducing language.

---

## 🧠 Models Evaluated
Five major LLMs were tested:
- GPT-3.5-turbo
- GPT-4
- GPT-4-turbo
- Claude-instant-1
- PaLM-2-CodeChat-bison

---

## 📊 Experimental Setup

### 🔄 Prompting Strategies
- **Zero-shot**
- **One-shot**
- **Few-shot**
- **Chain-of-Thought (CoT)**

### 🧪 Bias Detection Accuracy
- AST-based testing achieved:
  - **100% precision**
  - **92% recall**
- Human reviewers validated outputs with syntax/runtime errors.

---

## 📈 Key Findings

### 🔥 Bias Prevalence
- Bias is **widespread** across all models.
- GPT-4-turbo showed **84.13% age bias** across five runs.
- Claude-instant-1 had **49.10% gender bias**.
- Attributes with highest bias:
  - Region
  - Age
  - Gender
  - Education
- Attributes with lower bias:
  - Race
  - Occupation

### ⚠️ Prompt Engineering Limitations
- Prompt engineering alone (zero-shot, few-shot, CoT) had **limited success**.
- In some cases, it **increased bias** due to added complexity.

---

## 🛠️ Mitigation Techniques

### 1. **Prompt Engineering Alone**
- Tested various styles (zero-shot, few-shot, CoT).
- Result: Inconsistent and often ineffective.

### 2. **Prompting with Feedback**
- Feedback from bias testing is used to refine prompts.
- Models are re-prompted with instructions to correct biased behavior.
- Result: **Significant bias reduction**:
  - GPT-4 CBS dropped from **59.88% → 4.79%**
  - GPT-4-turbo CBS dropped from **76.05% → 0.30%**

---

## 🤖 LLM Self-Diagnosis Limitations
- GPT-3.5-turbo detected only **18.84%** of age biases in its own code.
- Feedback-based refinement outperforms self-diagnosis.

---

## 🔄 Fairness vs. Performance
- Trade-offs between fairness and task accuracy are acknowledged but not deeply explored.
- Future work may investigate how bias mitigation affects model utility.

---

## 🧩 Contributions
1. A novel **AST-based bias testing framework** for code generation.
2. Empirical evaluation of **five major LLMs** across **334 tasks**.
3. Introduction of **CBS, CBS_U@K, CBS_I@K** metrics.
4. Demonstration of **feedback-based mitigation** as a superior strategy.

---

## 🧭 Implications
- Biased code can have real-world consequences in domains like hiring, healthcare, and finance.
- Ethical and technical safeguards are essential for deploying LLMs in production environments.
- Scenario-based testing and feedback loops should be integrated into model development pipelines.

---

## 📚 Keywords
`LLM`, `Code Generation`, `Bias Detection`, `AST`, `Prompt Engineering`, `Feedback Refinement`, `Fairness`, `Ethics`, `CBS`, `Chain-of-Thought`, `RLHF`
