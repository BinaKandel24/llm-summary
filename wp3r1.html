<h1 id="bias-testing-and-mitigation-in-llm-based-code-generation">🧠
Bias Testing and Mitigation in LLM-based Code Generation</h1>
<h2 id="objective">🎯 Objective</h2>
<p>To investigate whether code generated by Large Language Models (LLMs)
exhibits <strong>social bias</strong>, and to develop a framework for
<strong>detecting and mitigating</strong> such bias. The study focuses
on biases related to: - Age - Gender - Race - Education - Occupation -
Region</p>
<hr />
<h2 id="methodology">🧪 Methodology</h2>
<h3 id="bias-testing-framework">🔍 Bias Testing Framework</h3>
<ul>
<li>Uses <strong>Abstract Syntax Trees (ASTs)</strong> to extract
function names, parameters, and values from generated code.</li>
<li>Constructs <strong>test cases</strong> by varying protected
attributes while keeping other inputs constant.</li>
<li>Evaluates whether the output changes inappropriately, indicating
bias.</li>
</ul>
<h3 id="bias-metrics">📐 Bias Metrics</h3>
<p>Three novel metrics are introduced: - <strong>CBS (Code Bias
Score)</strong>: Percentage of biased functions in a single run. -
<strong>CBS_U@K (Union at K)</strong>: Measures how many unique biased
outputs appear across K runs. - <strong>CBS_I@K (Intersection at
K)</strong>: Measures consistent bias across K runs.</p>
<h3 id="prompt-construction">🧰 Prompt Construction</h3>
<ul>
<li>Prompts are derived from three real-world tasks:
<ul>
<li><strong>Adult Income Prediction</strong></li>
<li><strong>Employability Assessment</strong></li>
<li><strong>Health Insurance Eligibility</strong></li>
</ul></li>
<li>Prompts are filtered to remove duplicates, irrelevant content, and
bias-inducing language.</li>
</ul>
<hr />
<h2 id="models-evaluated">🧠 Models Evaluated</h2>
<p>Five major LLMs were tested: - GPT-3.5-turbo - GPT-4 - GPT-4-turbo -
Claude-instant-1 - PaLM-2-CodeChat-bison</p>
<hr />
<h2 id="experimental-setup">📊 Experimental Setup</h2>
<h3 id="prompting-strategies">🔄 Prompting Strategies</h3>
<ul>
<li><strong>Zero-shot</strong></li>
<li><strong>One-shot</strong></li>
<li><strong>Few-shot</strong></li>
<li><strong>Chain-of-Thought (CoT)</strong></li>
</ul>
<h3 id="bias-detection-accuracy">🧪 Bias Detection Accuracy</h3>
<ul>
<li>AST-based testing achieved:
<ul>
<li><strong>100% precision</strong></li>
<li><strong>92% recall</strong></li>
</ul></li>
<li>Human reviewers validated outputs with syntax/runtime errors.</li>
</ul>
<hr />
<h2 id="key-findings">📈 Key Findings</h2>
<h3 id="bias-prevalence">🔥 Bias Prevalence</h3>
<ul>
<li>Bias is <strong>widespread</strong> across all models.</li>
<li>GPT-4-turbo showed <strong>84.13% age bias</strong> across five
runs.</li>
<li>Claude-instant-1 had <strong>49.10% gender bias</strong>.</li>
<li>Attributes with highest bias:
<ul>
<li>Region</li>
<li>Age</li>
<li>Gender</li>
<li>Education</li>
</ul></li>
<li>Attributes with lower bias:
<ul>
<li>Race</li>
<li>Occupation</li>
</ul></li>
</ul>
<h3 id="prompt-engineering-limitations">⚠️ Prompt Engineering
Limitations</h3>
<ul>
<li>Prompt engineering alone (zero-shot, few-shot, CoT) had
<strong>limited success</strong>.</li>
<li>In some cases, it <strong>increased bias</strong> due to added
complexity.</li>
</ul>
<hr />
<h2 id="mitigation-techniques">🛠️ Mitigation Techniques</h2>
<h3 id="prompt-engineering-alone">1. <strong>Prompt Engineering
Alone</strong></h3>
<ul>
<li>Tested various styles (zero-shot, few-shot, CoT).</li>
<li>Result: Inconsistent and often ineffective.</li>
</ul>
<h3 id="prompting-with-feedback">2. <strong>Prompting with
Feedback</strong></h3>
<ul>
<li>Feedback from bias testing is used to refine prompts.</li>
<li>Models are re-prompted with instructions to correct biased
behavior.</li>
<li>Result: <strong>Significant bias reduction</strong>:
<ul>
<li>GPT-4 CBS dropped from <strong>59.88% → 4.79%</strong></li>
<li>GPT-4-turbo CBS dropped from <strong>76.05% → 0.30%</strong></li>
</ul></li>
</ul>
<hr />
<h2 id="llm-self-diagnosis-limitations">🤖 LLM Self-Diagnosis
Limitations</h2>
<ul>
<li>GPT-3.5-turbo detected only <strong>18.84%</strong> of age biases in
its own code.</li>
<li>Feedback-based refinement outperforms self-diagnosis.</li>
</ul>
<hr />
<h2 id="fairness-vs.-performance">🔄 Fairness vs. Performance</h2>
<ul>
<li>Trade-offs between fairness and task accuracy are acknowledged but
not deeply explored.</li>
<li>Future work may investigate how bias mitigation affects model
utility.</li>
</ul>
<hr />
<h2 id="contributions">🧩 Contributions</h2>
<ol type="1">
<li>A novel <strong>AST-based bias testing framework</strong> for code
generation.</li>
<li>Empirical evaluation of <strong>five major LLMs</strong> across
<strong>334 tasks</strong>.</li>
<li>Introduction of <strong>CBS, CBS_U@K, CBS_I@K</strong> metrics.</li>
<li>Demonstration of <strong>feedback-based mitigation</strong> as a
superior strategy.</li>
</ol>
<hr />
<h2 id="implications">🧭 Implications</h2>
<ul>
<li>Biased code can have real-world consequences in domains like hiring,
healthcare, and finance.</li>
<li>Ethical and technical safeguards are essential for deploying LLMs in
production environments.</li>
<li>Scenario-based testing and feedback loops should be integrated into
model development pipelines.</li>
</ul>
<hr />
<h2 id="keywords">📚 Keywords</h2>
<p><code>LLM</code>, <code>Code Generation</code>,
<code>Bias Detection</code>, <code>AST</code>,
<code>Prompt Engineering</code>, <code>Feedback Refinement</code>,
<code>Fairness</code>, <code>Ethics</code>, <code>CBS</code>,
<code>Chain-of-Thought</code>, <code>RLHF</code></p>
