# ğŸ§  Bias Testing and Mitigation in LLM-based Code Generation

## ğŸ¯ Objective
To investigate whether code generated by Large Language Models (LLMs) exhibits **social bias**, and to develop a framework for **detecting and mitigating** such bias. The study focuses on biases related to:
- Age
- Gender
- Race
- Education
- Occupation
- Region

---

## ğŸ§ª Methodology

### ğŸ” Bias Testing Framework
- Uses **Abstract Syntax Trees (ASTs)** to extract function names, parameters, and values from generated code.
- Constructs **test cases** by varying protected attributes while keeping other inputs constant.
- Evaluates whether the output changes inappropriately, indicating bias.

### ğŸ“ Bias Metrics
Three novel metrics are introduced:
- **CBS (Code Bias Score)**: Percentage of biased functions in a single run.
- **CBS_U@K (Union at K)**: Measures how many unique biased outputs appear across K runs.
- **CBS_I@K (Intersection at K)**: Measures consistent bias across K runs.

### ğŸ§° Prompt Construction
- Prompts are derived from three real-world tasks:
  - **Adult Income Prediction**
  - **Employability Assessment**
  - **Health Insurance Eligibility**
- Prompts are filtered to remove duplicates, irrelevant content, and bias-inducing language.

---

## ğŸ§  Models Evaluated
Five major LLMs were tested:
- GPT-3.5-turbo
- GPT-4
- GPT-4-turbo
- Claude-instant-1
- PaLM-2-CodeChat-bison

---

## ğŸ“Š Experimental Setup

### ğŸ”„ Prompting Strategies
- **Zero-shot**
- **One-shot**
- **Few-shot**
- **Chain-of-Thought (CoT)**

### ğŸ§ª Bias Detection Accuracy
- AST-based testing achieved:
  - **100% precision**
  - **92% recall**
- Human reviewers validated outputs with syntax/runtime errors.

---

## ğŸ“ˆ Key Findings

### ğŸ”¥ Bias Prevalence
- Bias is **widespread** across all models.
- GPT-4-turbo showed **84.13% age bias** across five runs.
- Claude-instant-1 had **49.10% gender bias**.
- Attributes with highest bias:
  - Region
  - Age
  - Gender
  - Education
- Attributes with lower bias:
  - Race
  - Occupation

### âš ï¸ Prompt Engineering Limitations
- Prompt engineering alone (zero-shot, few-shot, CoT) had **limited success**.
- In some cases, it **increased bias** due to added complexity.

---

## ğŸ› ï¸ Mitigation Techniques

### 1. **Prompt Engineering Alone**
- Tested various styles (zero-shot, few-shot, CoT).
- Result: Inconsistent and often ineffective.

### 2. **Prompting with Feedback**
- Feedback from bias testing is used to refine prompts.
- Models are re-prompted with instructions to correct biased behavior.
- Result: **Significant bias reduction**:
  - GPT-4 CBS dropped from **59.88% â†’ 4.79%**
  - GPT-4-turbo CBS dropped from **76.05% â†’ 0.30%**

---

## ğŸ¤– LLM Self-Diagnosis Limitations
- GPT-3.5-turbo detected only **18.84%** of age biases in its own code.
- Feedback-based refinement outperforms self-diagnosis.

---

## ğŸ”„ Fairness vs. Performance
- Trade-offs between fairness and task accuracy are acknowledged but not deeply explored.
- Future work may investigate how bias mitigation affects model utility.

---

## ğŸ§© Contributions
1. A novel **AST-based bias testing framework** for code generation.
2. Empirical evaluation of **five major LLMs** across **334 tasks**.
3. Introduction of **CBS, CBS_U@K, CBS_I@K** metrics.
4. Demonstration of **feedback-based mitigation** as a superior strategy.

---

## ğŸ§­ Implications
- Biased code can have real-world consequences in domains like hiring, healthcare, and finance.
- Ethical and technical safeguards are essential for deploying LLMs in production environments.
- Scenario-based testing and feedback loops should be integrated into model development pipelines.

---

## ğŸ“š Keywords
`LLM`, `Code Generation`, `Bias Detection`, `AST`, `Prompt Engineering`, `Feedback Refinement`, `Fairness`, `Ethics`, `CBS`, `Chain-of-Thought`, `RLHF`
