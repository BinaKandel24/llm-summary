<h1
id="bias-testing-and-mitigation-in-llm-based-code-generation-objective-to-investigate-whether-code-generated-by-large-language-models-llms-exhibits-social-bias-and-to-develop-a-framework-for-detecting-and-mitigating-such-bias.-the-study-focuses-on-biases-related-to---age---gender---race---education---occupation---region-methodology-bias-testing-framework---uses-abstract-syntax-trees-asts-to-extract-function-names-parameters-and-values-from-generated-code.---constructs-test-cases-by-varying-protected-attributes-while-keeping-other-inputs-constant.---evaluates-whether-the-output-changes-inappropriately-indicating-bias.-bias-metrics-three-novel-metrics-are-introduced---cbs-code-bias-score-percentage-of-biased-functions-in-a-single-run.---cbs_uk-union-at-k-measures-how-many-unique-biased-outputs-appear-across-k-runs.---cbs_ik-intersection-at-k-measures-consistent-bias-across-k-runs.-prompt-construction---prompts-are-derived-from-three-real-world-tasks---adult-income-prediction---employability-assessment---health-insurance-eligibility---prompts-are-filtered-to-remove-duplicates-irrelevant-content-and-bias-inducing-language.-models-evaluated-five-major-llms-were-tested---gpt-3.5-turbo---gpt-4---gpt-4-turbo---claude-instant-1---palm-2-codechat-bison-experimental-setup-prompting-strategies---zero-shot---one-shot---few-shot---chain-of-thought-cot-bias-detection-accuracy---ast-based-testing-achieved---100-precision---92-recall---human-reviewers-validated-outputs-with-syntaxruntime-errors.-key-findings-bias-prevalence---bias-is-widespread-across-all-models.---gpt-4-turbo-showed-84.13-age-bias-across-five-runs.---claude-instant-1-had-49.10-gender-bias.---attributes-with-highest-bias---region---age---gender---education---attributes-with-lower-bias---race---occupation-prompt-engineering-limitations---prompt-engineering-alone-zero-shot-few-shot-cot-had-limited-success.---in-some-cases-it-increased-bias-due-to-added-complexity.-mitigation-techniques-1.-prompt-engineering-alone---tested-various-styles-zero-shot-few-shot-cot.---result-inconsistent-and-often-ineffective.-2.-prompting-with-feedback---feedback-from-bias-testing-is-used-to-refine-prompts.---models-are-re-prompted-with-instructions-to-correct-biased-behavior.---result-significant-bias-reduction---gpt-4-cbs-dropped-from-59.88-4.79---gpt-4-turbo-cbs-dropped-from-76.05-0.30-llm-self-diagnosis-limitations---gpt-3.5-turbo-detected-only-18.84-of-age-biases-in-its-own-code.---feedback-based-refinement-outperforms-self-diagnosis.-fairness-vs.-performance---trade-offs-between-fairness-and-task-accuracy-are-acknowledged-but-not-deeply-explored.---future-work-may-investigate-how-bias-mitigation-affects-model-utility.-contributions-1.-a-novel-ast-based-bias-testing-framework-for-code-generation.-2.-empirical-evaluation-of-five-major-llms-across-334-tasks.-3.-introduction-of-cbs-cbs_uk-cbs_ik-metrics.-4.-demonstration-of-feedback-based-mitigation-as-a-superior-strategy.-implications---biased-code-can-have-real-world-consequences-in-domains-like-hiring-healthcare-and-finance.---ethical-and-technical-safeguards-are-essential-for-deploying-llms-in-production-environments.---scenario-based-testing-and-feedback-loops-should-be-integrated-into-model-development-pipelines.-keywords-llm-code-generation-bias-detection-ast-prompt-engineering-feedback-refinement-fairness-ethics-cbs-chain-of-thought-rlhf">ğŸ§ 
Bias Testing and Mitigation in LLM-based Code Generation ## ğŸ¯ Objective
To investigate whether code generated by Large Language Models (LLMs)
exhibits <strong>social bias</strong>, and to develop a framework for
<strong>detecting and mitigating</strong> such bias. The study focuses
on biases related to: - Age - Gender - Race - Education - Occupation -
Region â€” ## ğŸ§ª Methodology ### ğŸ” Bias Testing Framework - Uses
<strong>Abstract Syntax Trees (ASTs)</strong> to extract function names,
parameters, and values from generated code. - Constructs <strong>test
cases</strong> by varying protected attributes while keeping other
inputs constant. - Evaluates whether the output changes inappropriately,
indicating bias. ### ğŸ“ Bias Metrics Three novel metrics are introduced:
- <strong>CBS (Code Bias Score)</strong>: Percentage of biased functions
in a single run. - <strong>CBS_U@K (Union at K)</strong>: Measures how
many unique biased outputs appear across K runs. - <strong>CBS_I@K
(Intersection at K)</strong>: Measures consistent bias across K runs.
### ğŸ§° Prompt Construction - Prompts are derived from three real-world
tasks: - <strong>Adult Income Prediction</strong> -
<strong>Employability Assessment</strong> - <strong>Health Insurance
Eligibility</strong> - Prompts are filtered to remove duplicates,
irrelevant content, and bias-inducing language. â€” ## ğŸ§  Models Evaluated
Five major LLMs were tested: - GPT-3.5-turbo - GPT-4 - GPT-4-turbo -
Claude-instant-1 - PaLM-2-CodeChat-bison â€” ## ğŸ“Š Experimental Setup ###
ğŸ”„ Prompting Strategies - <strong>Zero-shot</strong> -
<strong>One-shot</strong> - <strong>Few-shot</strong> -
<strong>Chain-of-Thought (CoT)</strong> ### ğŸ§ª Bias Detection Accuracy -
AST-based testing achieved: - <strong>100% precision</strong> -
<strong>92% recall</strong> - Human reviewers validated outputs with
syntax/runtime errors. â€” ## ğŸ“ˆ Key Findings ### ğŸ”¥ Bias Prevalence -
Bias is <strong>widespread</strong> across all models. - GPT-4-turbo
showed <strong>84.13% age bias</strong> across five runs. -
Claude-instant-1 had <strong>49.10% gender bias</strong>. - Attributes
with highest bias: - Region - Age - Gender - Education - Attributes with
lower bias: - Race - Occupation ### âš ï¸ Prompt Engineering Limitations -
Prompt engineering alone (zero-shot, few-shot, CoT) had <strong>limited
success</strong>. - In some cases, it <strong>increased bias</strong>
due to added complexity. â€” ## ğŸ› ï¸ Mitigation Techniques ### 1.
<strong>Prompt Engineering Alone</strong> - Tested various styles
(zero-shot, few-shot, CoT). - Result: Inconsistent and often
ineffective. ### 2. <strong>Prompting with Feedback</strong> - Feedback
from bias testing is used to refine prompts. - Models are re-prompted
with instructions to correct biased behavior. - Result:
<strong>Significant bias reduction</strong>: - GPT-4 CBS dropped from
<strong>59.88% â†’ 4.79%</strong> - GPT-4-turbo CBS dropped from
<strong>76.05% â†’ 0.30%</strong> â€” ## ğŸ¤– LLM Self-Diagnosis Limitations -
GPT-3.5-turbo detected only <strong>18.84%</strong> of age biases in its
own code. - Feedback-based refinement outperforms self-diagnosis. â€” ##
ğŸ”„ Fairness vs. Performance - Trade-offs between fairness and task
accuracy are acknowledged but not deeply explored. - Future work may
investigate how bias mitigation affects model utility. â€” ## ğŸ§©
Contributions 1. A novel <strong>AST-based bias testing
framework</strong> for code generation. 2. Empirical evaluation of
<strong>five major LLMs</strong> across <strong>334 tasks</strong>. 3.
Introduction of <strong>CBS, CBS_U@K, CBS_I@K</strong> metrics. 4.
Demonstration of <strong>feedback-based mitigation</strong> as a
superior strategy. â€” ## ğŸ§­ Implications - Biased code can have
real-world consequences in domains like hiring, healthcare, and finance.
- Ethical and technical safeguards are essential for deploying LLMs in
production environments. - Scenario-based testing and feedback loops
should be integrated into model development pipelines. â€” ## ğŸ“š Keywords
<code>LLM</code>, <code>Code Generation</code>,
<code>Bias Detection</code>, <code>AST</code>,
<code>Prompt Engineering</code>, <code>Feedback Refinement</code>,
<code>Fairness</code>, <code>Ethics</code>, <code>CBS</code>,
<code>Chain-of-Thought</code>, <code>RLHF</code></h1>
