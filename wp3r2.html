<h1
id="securing-large-language-models-addressing-bias-misinformation-and-prompt-attacks">SECURING
LARGE LANGUAGE MODELS: ADDRESSING BIAS, MISINFORMATION, AND PROMPT
ATTACKS</h1>
<h2 id="authors">Authors</h2>
<ul>
<li><strong>Benji Peng</strong> (Research Scientist, AppCubic) -
benji@appcubic.com</li>
<li><strong>Keyu Chen</strong> (Georgia Institute of Technology) -
kchen637@gatech.edu</li>
<li><strong>Ming Li</strong> (Georgia Institute of Technology) -
mli694@gatech.edu</li>
<li><strong>Pohsun Feng</strong> (National Taiwan Normal University) -
41075018h@ntnu.edu.tw</li>
<li><strong>Ziqian Bi</strong> (Indiana University) - bizi@iu.edu</li>
<li><strong>Junyu Liu</strong> (Kyoto University) -
liu.junyu.82w@st.kyoto-u.ac.jp</li>
<li><strong>Qian Niu</strong>* (Kyoto University) -
niu.qian.f44@kyoto-u.jp</li>
</ul>
<h2 id="abstract">Abstract</h2>
<p>Large Language Models (LLMs) demonstrate impressive capabilities
across various fields, yet their increasing use raises critical security
concerns. This article reviews recent literature addressing key issues
in LLM security, with a focus on:</p>
<ul>
<li><strong>Accuracy and Misinformation</strong>: Issues related to
inaccurate or misleading outputs from LLMs, with emphasis on
implementing fact-checking methodologies to enhance response
reliability</li>
<li><strong>Bias</strong>: Inherent biases within LLMs examined through
diverse evaluation techniques, including controlled input studies and
red teaming exercises</li>
<li><strong>Content Detection</strong>: The complexity of distinguishing
LLM-generated content from human-produced text, introducing detection
mechanisms like DetectGPT and watermarking techniques</li>
<li><strong>Vulnerabilities</strong>: LLM vulnerabilities including
jailbreak attacks and prompt injection exploits, analyzed through case
studies and competitions like HackAPrompt</li>
</ul>
<h2 id="introduction">1. Introduction</h2>
<h3 id="llm-capabilities-and-applications">LLM Capabilities and
Applications</h3>
<p>Large Language Models represent one of the most transformative
technologies in AI, leveraging vast datasets and cutting-edge neural
network architectures such as Transformers. Their capabilities
include:</p>
<p><strong>Core Functions:</strong> - Understanding, generating, and
manipulating human language with unprecedented sophistication - Text
generation and conversation systems - Multi-modal tasks integrating
modalities beyond language - Autonomous agents capable of complex
decision-making - Content understanding across diverse data sources</p>
<p><strong>Applications:</strong> - AI-driven customer support -
Automated coding - Virtual assistants - Intelligent systems for
industrial automation - Medical diagnostics - Autonomous vehicles -
Cross-lingual understanding - Multimodal data integration</p>
<h3 id="security-challenges">Security Challenges</h3>
<p>The widespread deployment of LLMs has introduced significant security
challenges:</p>
<ol type="1">
<li><strong>Misinformation Generation</strong>: LLMs frequently generate
incorrect or hallucinated outputs due to inherent limitations in
training data or contextual misunderstandings</li>
<li><strong>Bias Perpetuation</strong>: Models often perpetuate or
amplify societal stereotypes and political imbalances present in
training data</li>
<li><strong>Content Detection Challenges</strong>: Difficulty in
differentiating human-generated from LLM-generated content</li>
<li><strong>Security Vulnerabilities</strong>: Susceptibility to
adversarial attacks including prompt injection and jailbreaking</li>
</ol>
<h2 id="detecting-hallucination">2. Detecting Hallucination</h2>
<h3 id="root-causes-of-hallucination">Root Causes of Hallucination</h3>
<p>LLMs hallucinate because they: - Rely on statistical patterns within
word embeddings rather than true cognitive processes - Predict the next
most likely word based on training data patterns without understanding
factual accuracy - Generate coherent-sounding but false information when
lacking sufficient factual context</p>
<h3 id="multimodal-hallucination-causes">Multimodal Hallucination
Causes</h3>
<p><strong>Data-Related Issues:</strong> - Insufficient or noisy data -
Statistical biases leading to misalignment between visual and textual
inputs</p>
<p><strong>Model-Related Issues:</strong> - Weak vision models -
Over-reliance on language knowledge - Poor cross-modal interfaces
hindering accurate information integration</p>
<p><strong>Training-Related Issues:</strong> - Ineffective loss
functions - Absence of human feedback</p>
<p><strong>Inference-Related Issues:</strong> - Loss of visual focus
during generation</p>
<h3 id="detection-methods">Detection Methods</h3>
<h4 id="llm-based-detection">LLM-Based Detection</h4>
<ul>
<li><strong>GPT-4 as Detector</strong>: Zero-shot learning approach
where LLMs assess hallucination without prior fine-tuning</li>
<li><strong>Chain-of-Thought (CoT) Prompting</strong>: Guides models to
generate reasoning steps for structured evaluation</li>
<li><strong>HaluEval</strong>: Uses automatic sampling and human
annotation for benchmarking</li>
</ul>
<h4 id="embedding-based-detection">Embedding-Based Detection</h4>
<ul>
<li><strong>Semantic Comparison</strong>: Generates embeddings of model
outputs and trusted data, using techniques like t-SNE projections</li>
<li><strong>Limitation</strong>: Less effective when generated
misinformation closely mimics factual content structure</li>
</ul>
<h4 id="classification-based-detection">Classification-Based
Detection</h4>
<ul>
<li><strong>MIND (Modeling INternal-states for hallucination
Detection)</strong>: Unsupervised method using labeled datasets</li>
<li><strong>Feature Analysis</strong>: Evaluates textual features like
factual inconsistencies, contradictions, and stylistic anomalies</li>
<li><strong>Logit-Based Scoring</strong>: Uses logit outputs to assess
token/phrase accuracy</li>
<li><strong>Ensemble Methods</strong>: Combines multiple models
(FactSumm, Smart, SummaC, SelfcheckGPT) for improved robustness</li>
</ul>
<h4 id="external-knowledge-integration">External Knowledge
Integration</h4>
<ul>
<li><strong>Retrieval-Augmented Generation (RAG)</strong>: Incorporates
external, real-time factual sources during generation</li>
<li><strong>Success Dependency</strong>: Relies on quality and relevance
of retrieved data and model’s integration ability</li>
</ul>
<h3 id="improving-output-accuracy">2.1 Improving Output Accuracy</h3>
<h4 id="fact-checking-mechanisms">Fact-Checking Mechanisms</h4>
<p><strong>FACTOOL:</strong> - Integrates external tools to verify
factual accuracy - Breaks down complex tasks into smaller claims -
Checks against sources like search engines or research databases -
Provides real-time evidence for validation</p>
<p><strong>FACTSCORE:</strong> - Divides long-form text into atomic
facts - Independently checks each atomic unit against reliable sources -
Enables finer-level accuracy assessment - Helpful when sentences contain
both true and false information</p>
<p><strong>Limitations:</strong> - Performance loss in large-scale,
open-ended text generation - Difficulty fact-checking against constantly
evolving knowledge sources</p>
<h4 id="knowledge-integration-solutions">Knowledge Integration
Solutions</h4>
<p><strong>LLM-Augmenter:</strong> - Integrates external knowledge
through Plug-and-Play (PnP) modules - Retrieves relevant data from
external sources - Iteratively revises outputs when hallucinations are
detected</p>
<p><strong>FreshPrompt:</strong> - In-context learning method addressing
static/outdated information - Uses one-shot prompting with real-time
search engine data - Ensures responses remain current and up-to-date</p>
<h2 id="built-in-bias-in-llms">3. Built-in Bias in LLMs</h2>
<h3 id="types-of-bias">Types of Bias</h3>
<h4 id="source-bias">Source Bias</h4>
<ul>
<li>Neural retrieval models show systematic preference for LLM-generated
content over human-written text</li>
<li>Stems from higher semantic coherence and lower perplexity of
LLM-generated content</li>
</ul>
<h4 id="political-bias">Political Bias</h4>
<ul>
<li>Conversational LLMs (GPT-4, Claude) show consistent left-leaning
bias in politically charged questions</li>
<li>Base models without supervised fine-tuning display less clear
political leanings</li>
<li>Bias often introduced through training data or fine-tuning
processes</li>
</ul>
<h4 id="implicit-bias">Implicit Bias</h4>
<ul>
<li>Models passing explicit bias tests still contain implicit
biases</li>
<li>Rooted in societal stereotypes</li>
<li>Potential to lead to discrimination in real-world applications</li>
</ul>
<h4 id="geographic-bias">Geographic Bias</h4>
<ul>
<li>LLMs favor regions with higher socioeconomic conditions</li>
<li>Reflects biases inherent in training data</li>
<li>Leads to inaccurate predictions and discriminatory outcomes in
healthcare and law</li>
</ul>
<h4 id="gender-bias">Gender Bias</h4>
<ul>
<li>Reflects gender stereotypes in occupational classification
tasks</li>
<li>May be mitigated through Chain-of-Thought (CoT) prompting</li>
<li>CoT encourages articulation of reasoning for improved
decision-making</li>
</ul>
<h3 id="bias-detection-methods">Bias Detection Methods</h3>
<h4 id="prompt-based-methods">Prompt-Based Methods</h4>
<ul>
<li>Inspired by Implicit Association Test (IAT)</li>
<li>Utilize crafted prompts to elicit biased responses</li>
</ul>
<h4 id="embedding-based-methods">Embedding-Based Methods</h4>
<ul>
<li><strong>Word Embedding Association Test (WEAT)</strong>: Assesses
biases in word embeddings</li>
<li><strong>Sentence Embedding Association Test (SEAT)</strong>:
Evaluates sentence embedding biases</li>
<li>Help understand underlying representations learned by LLMs</li>
</ul>
<h4 id="generation-based-methods">Generation-Based Methods</h4>
<ul>
<li>Focus on analyzing LLM-generated text</li>
<li>Evaluate biases in content, language choices, and overall
sentiment</li>
</ul>
<h4 id="red-teaming">Red Teaming</h4>
<ul>
<li>Uses other LLMs to generate test cases provoking harmful
behaviors</li>
<li>Proactive method for identifying potential model risks before
deployment</li>
</ul>
<h3 id="bias-mitigation-strategies">3.1 Bias Mitigation Strategies</h3>
<h4 id="pre-processing-stage">Pre-Processing Stage</h4>
<p><strong>Data Augmentation:</strong> - <strong>Counterfactual Data
Augmentation (CDA)</strong>: Balances datasets by substituting
attributes related to gender, race, or protected groups -
<strong>Counterfactual Data Substitution (CDS)</strong>: Improved CDA
approach randomly replacing attributes to mitigate bias</p>
<p><strong>Prompt Tuning:</strong> - Encourages neutral or less
stereotypical outputs by adjusting input prompts - <strong>Hard
Prompts</strong>: Use static templates - <strong>Soft Prompts</strong>:
Generate embeddings dynamically during model interactions</p>
<h4 id="in-training-stage">In-Training Stage</h4>
<p><strong>Iterative Null Space Projection (INLP):</strong> - Removes
bias by projecting targeted attributes into spaces where they don’t
influence outputs</p>
<p><strong>Causal Regularization:</strong> - Ensures models rely on
meaningful, causal relationships rather than biased correlations</p>
<p><strong>Auxiliary Modules:</strong> - <strong>Adapter-based Debiasing
(ADELE)</strong>: Uses additional modules to address bias without
retraining entire model - <strong>GEnder Equality Prompt
(GEEP)</strong>: Overcomes catastrophic forgetting, improves gender
fairness by freezing pre-trained model and learning gender-related
prompts with gender-neutral data</p>
<h4 id="intra-processing-stage">Intra-Processing Stage</h4>
<p><strong>Model Editing:</strong> - Enables targeted updates to model
behavior - Corrects biases in specific areas without affecting overall
performance</p>
<p><strong>Decoding Modification:</strong> - <strong>DEXPERTS</strong>:
Directly affects text generation by adjusting token probabilities - Uses
two models: one promoting non-toxic text, another discouraging harmful
content</p>
<h4 id="post-processing-stage">Post-Processing Stage</h4>
<p><strong>Chain-of-Thought (CoT) Prompting:</strong> - Guides models
through logical reasoning steps - Ensures unbiased response generation -
Reduces biases in gender and occupation-related tasks</p>
<p><strong>Rewriting:</strong> - Detects and replaces biased outputs
with neutral language - Reduces content bias after generation</p>
<h2 id="detecting-llm-generated-content">4. Detecting LLM-Generated
Content</h2>
<p>The proliferation of LLMs blurs the line between human-written and
AI-generated content, raising concerns about information integrity.
Detection methods fall into three broad categories:</p>
<h3 id="metric-based-approaches">4.1 Metric-Based Approaches</h3>
<p>These methods detect AI-generated text based on inherent statistical
properties of LLM outputs, relying on distributional features within the
model’s probability space.</p>
<p><strong>DetectGPT:</strong> - Proposed by Mitchell et al. - Exploits
negative curvature in probability space of generated text - Provides
zero-shot detection mechanism - <strong>Limitation</strong>:
Effectiveness remains limited</p>
<p><strong>Intrinsic Dimensionality:</strong> - Measures complexity of
text to detect LLM-generated content - Human-written content typically
exhibits higher dimensionality due to diversity and creativity</p>
<h3 id="model-based-approaches">4.2 Model-Based Approaches</h3>
<p>Utilize supervised learning to identify AI-generated text through
classifiers trained on labeled datasets from both AI-generated and
human-generated categories.</p>
<p><strong>Major Issues:</strong> - <strong>Generalization
Problems</strong>: Classifiers fail with new LLM architectures or
unfamiliar domains - <strong>Manipulation Vulnerability</strong>: Poor
performance with paraphrasing and manual editing - <strong>Bias
Issues</strong>: Disproportionately flag text from non-native speakers
as machine-generated</p>
<h3 id="watermarking-and-embedded-signal-approaches">4.3 Watermarking
and Embedded Signal Approaches</h3>
<p>Offer alternatives to metric-based and model-based limitations by
embedding detectable signals directly within LLM outputs.</p>
<p><strong>Soft Watermarking:</strong> - Introduced by Kirchenbauer et
al. - Biases language model to select from specific token subsets during
generation - Creates detectable statistical patterns in output -
<strong>Vulnerability</strong>: Susceptible to paraphrasing - small
wording changes disrupt token patterns</p>
<p><strong>Retrieval-Based Detection:</strong> - Stores generated text
in databases for future comparison - Uses similarity searches to
identify underlying similarities - Less vulnerable to paraphrasing than
token-based methods - <strong>Privacy Concerns</strong>: Requires
storing large amounts of user-generated content</p>
<h3 id="additional-challenges">4.4 Additional Challenges</h3>
<h4 id="adversarial-attacks">Adversarial Attacks</h4>
<ul>
<li><strong>Spoofing</strong>: Attackers craft human-written text
mimicking AI-generated statistical patterns, causing false
positives</li>
<li><strong>Persona Impersonation</strong>: LLMs aligned with personal
biases generate content tailored to specific personas, bypassing
detection methods</li>
</ul>
<h4 id="ethical-concerns">Ethical Concerns</h4>
<ul>
<li>Manipulation of LLMs for deceptive purposes raises broader ethical
issues</li>
<li>Need for balanced approach between detection accuracy and privacy
protection</li>
</ul>
<h2 id="jailbreaking-and-prompt-injection-in-large-language-models">5.
Jailbreaking and Prompt Injection in Large Language Models</h2>
<p>Jailbreaking and prompt injection represent significant security
challenges, threatening the integrity of LLM safety systems.</p>
<h3 id="definitions">Definitions</h3>
<ul>
<li><strong>Jailbreaking</strong>: Crafts specific inputs/prompts
bypassing model safety restrictions, generating outputs violating
pre-defined guidelines</li>
<li><strong>Prompt Injection</strong>: Manipulates models by embedding
malicious instructions within input prompts, hijacking intended
functions</li>
</ul>
<h3 id="defense-mechanisms">Defense Mechanisms</h3>
<p><strong>LLM Self Defense:</strong> - Introduces defense mechanism
relying on LLM self-examination - Involves querying LLM about
harmfulness of its own generated text - Demonstrates significant promise
in reducing attack success rates</p>
<p><strong>Bergeron Method:</strong> - Uses auxiliary model for
alignment checks - More effective than existing methods like OpenAI
Moderation API</p>
<h3 id="jailbreaking-exploiting-llm-vulnerabilities">5.1 Jailbreaking:
Exploiting LLM Vulnerabilities</h3>
<h4 id="evolution-of-jailbreak-techniques">Evolution of Jailbreak
Techniques</h4>
<ul>
<li><strong>Progressive Development</strong>: From straightforward
single-step manipulations to sophisticated multi-step approaches</li>
<li><strong>Complex Strategies</strong>: Involve prompt injection and
privilege escalation</li>
<li><strong>Human Communication Exploitation</strong>: Capitalize on
model’s understanding of human communication using adversarial
input</li>
</ul>
<h4 id="research-findings">Research Findings</h4>
<p><strong>JAILBREAKHUB Framework:</strong> - Analyzed over 1,400
prompts - Revealed increased complexity and effectiveness of modern
jailbreak strategies - <strong>Platform Facilitation</strong>: Online
platforms where prompts are shared, refined, and tested</p>
<p><strong>Model Vulnerability:</strong> - Even robust models like GPT-4
show significant vulnerability - Success rates as high as 90% despite
extensive safety mechanisms - <strong>Root Cause</strong>: Models’
inherent capacity to process human-like reasoning and persuasive
language</p>
<h4 id="crowdsourcing-evolution">Crowdsourcing Evolution</h4>
<ul>
<li><strong>Platforms</strong>: Reddit, Discord, dedicated
prompt-aggregation websites serve as hubs</li>
<li><strong>Community Effort</strong>: Disseminating and optimizing
jailbreak attacks through collaborative efforts</li>
<li><strong>Defense Inadequacy</strong>: Current internal and external
defenses insufficient against growing attack sophistication</li>
</ul>
<h3 id="prompt-injection-exploiting-llm-input-mechanisms">5.2 Prompt
Injection: Exploiting LLM Input Mechanisms</h3>
<p>Prompt injection manipulates LLM input mechanisms to alter output
generation in unintended ways, posing serious risks.</p>
<h4 id="attack-methods">Attack Methods</h4>
<p><strong>Template-Based Techniques:</strong> - <strong>78-Template
Approach</strong>: Widely recognized method particularly effective at
bypassing safeguards - <strong>High Success Rates</strong>: Achieve up
to 100% success under certain conditions across multiple LLMs (GPT-3.5,
Vicuna)</p>
<p><strong>Generative Methods:</strong> - <strong>GPTFuzz</strong>:
Demonstrates model susceptibility through automatically crafted complex
attack prompts - <strong>Advanced Manipulation</strong>: Shows
sophisticated adversarial prompt generation capabilities</p>
<h4 id="impact-on-model-safety">Impact on Model Safety</h4>
<ul>
<li>Results in biased, offensive, or privacy-violating outputs</li>
<li>Raises serious concerns about responsible LLM deployment</li>
<li>Challenges existing safety frameworks and guidelines</li>
</ul>
<h4 id="training-data-extraction">Training Data Extraction</h4>
<p><strong>Carlini et al. Investigation:</strong> - Explores how
attackers extract sensitive information (personal identifiers,
proprietary data) from training corpus - <strong>Attack Method</strong>:
Carefully designed prompts elicit memorized information directly from
model - <strong>Risk Factor</strong>: Particularly dangerous when LLMs
trained on vast amounts of unfiltered scraped data</p>
<p><strong>Cui et al. Broader Implications:</strong> - <strong>Privacy
Compromise</strong>: Data leakage vulnerabilities compromise privacy and
erode trust - <strong>Solution Requirements</strong>: Need for robust
privacy-preserving techniques like differential privacy or secure model
training - <strong>Prevention Focus</strong>: Ensuring sensitive data
doesn’t inadvertently leak through model interactions</p>
<h2 id="future-directions">6. Future Directions</h2>
<h3 id="hallucination-research-limitations">Hallucination Research
Limitations</h3>
<p><strong>Current Challenges:</strong> - Limited dataset diversity
focusing on narrow tasks with poor generalization - Lack of real-time
detection methods (most techniques address hallucinations
post-generation) - Integration challenges with external knowledge in
retrieval-augmented models - Underexplored cross-lingual and multimodal
hallucinations - Lack of interpretability impacting user trust</p>
<p><strong>Future Research Priorities:</strong> - Broader, more diverse
datasets - Real-time detection capabilities - Improved external
knowledge integration - More interpretable model architectures -
Enhanced cross-lingual and multimodal capabilities</p>
<h3 id="bias-research-gaps">Bias Research Gaps</h3>
<p><strong>Current Focus Limitations:</strong> - Primary focus on
gender, race, religion, and socioeconomic status - Other important
social biases relatively unexplored - Need for comprehensive approach to
understanding various bias encodings</p>
<p><strong>Key Challenges:</strong> - <strong>Performance
Balance</strong>: Finding right balance between bias reduction and
maintaining model performance - <strong>Multi-modal
Considerations</strong>: Need to investigate bias manifestation in text
and visual data processing systems - <strong>Real-World Impact
Assessment</strong>: Evaluating actual impact of biased models in
decision-making applications</p>
<p><strong>Ethical Considerations:</strong> - Transparency requirements
- Accountability mechanisms - Prevention of potential harm</p>
<h3 id="llm-security-gaps">LLM Security Gaps</h3>
<p><strong>Current Safety Training Inadequacies:</strong> - Existing
methods fall short, requiring fundamental shift in training approaches -
Need for defense mechanisms matching LLM sophistication - Requirement
for deeper understanding of information processing and adversarial input
handling</p>
<p><strong>Defense Requirements:</strong> -
<strong>Flexibility</strong>: More adaptive defenses to counter evolving
threats - <strong>Comprehensiveness</strong>: Ability to handle complex
attack combinations - <strong>Broad Applicability</strong>:
Effectiveness across various attack types and advanced models</p>
<p><strong>Evaluation Framework Needs:</strong> - Comprehensive
assessment of defense effectiveness against wide spectrum of risks -
Ensuring practical viability in real-world scenarios - Standardized
benchmarking for security measures</p>
<h2 id="keywords">Keywords</h2>
<p>LLM Security · Bias in LLMs · LLM Output Detection · Jailbreak
Attacks · Prompt Injection</p>
